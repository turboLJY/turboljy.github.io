<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html manifest="welcome.manifest">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta content="yes" name="apple-mobile-web-app-capable">
  <meta content="black" name="apple-mobile-web-app-status-bar-style">
  <meta content="telephone=no" name="format-detection">
  <meta name="author" content="Junyi Li">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #07889b; /*#1772d0;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #e37222; /*#f7b733;*/ /*f09228;*/
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    strong {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    heading {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 22px;
    color: #e37222; /*#fc4a1a;*/
    }
    heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 18px;
    }
    papertitle {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    font-weight: 700;
    }
    name {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link href="https://tuchuang-1258543525.cos.ap-beijing.myqcloud.com/20180614_161325021_iOS.jpg" rel="Shortcut Icon" type="image/x-icon">
  <title>Junyi Li (李军毅)</title>

  <link href="./stylesheets/main.css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
/*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

</head>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Junyi Li (李军毅)</name><br>
        </p>
        <p style="text-align:justify">
		I have a braod interest in natural language processing. My major 
		research interests lies in the area of Natural Language Generation, especially based on pretrained language models.
	<p>
	<p style="text-align:justify">
		Currently, I am a Ph.D. student from <a href="http://ai.ruc.edu.cn/english/index.htm">GSAI</a>, 
		<a href="https://ruc.edu.cn">Renmin University of China</a>, 
		advised by <a href="http://playbigdata.ruc.edu.cn/batmanfly/">Prof. Wayne Xin Zhao</a>. 
	</p>
	<p style="text-align:justify">
		Meanwhile, I was selected to participate
		in the joint doctral program of <a href="https://ruc.edu.cn">Renmin University of China</a> 
		and <a href="https://www.umontreal.ca/en/">University of Montreal</a>. 
		Starting in 2022, I will study in UdeM for my second Ph.D. degree.
        </p>
	<p>Email: lijunyi at ruc dot edu dot cn</p>
<!--         <p>
        <i>There is no authority in science. No one can tell whether your research matters or not. Or how much it matters. All you can do is to contribute to human knowledge and hope it will matter. Even if it doesn't, it does. It eliminates an idea.</i><br>
        </p>
		<p align="right">
		<i>-- Rich Sutton</i>
		</p> -->


        <p align="center">
			<a href="./images/resume.pdf" target="_blank">CV</a> &nbsp;/&nbsp;
			<a href="https://scholar.google.com/citations?user=zeWrn-4AAAAJ&hl=zh-CN" target="_blank">Google Scholar</a> &nbsp;/&nbsp;
			<a href="https://github.com/turboLJY" target="_blank"> GitHub </a> &nbsp;/&nbsp;
		    <a href="https://www.zhihu.com/people/li-jun-yi-93" target="_blank">Zhihu</a>
        </p>
        </td>
        <td width="33%">
        <img src="./images/lijunyi.jpg" width="95%">
        </td>
      </tr>
  </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>

        <tr><td>
            <heading>Zhihu Posts</heading>
            <ul>
	      <li> <a href="https://zhuanlan.zhihu.com/p/353972216" target="_blank">Knowledge-Enhanced Text Generation: 知识增强的文本生成研究进展</a></li>
              <li> <a href="https://zhuanlan.zhihu.com/p/69069509" target="_blank">ACL 2019 | 渐入佳境，基于主题感知的Coarse-to-Fine机制的在线评论生成</a></li>
			  <li> <a href="https://zhuanlan.zhihu.com/p/36880287" target="_blank">GAN+文本生成：让文本以假乱真</a></li>
			  
              <li> <a href="javascript:toggle_vis('zhihu-blogs')" style="color:black">show more</a> </li>
				  <div id="zhihu-blogs" style="display:none"> 
		   		  <li> <a href="https://zhuanlan.zhihu.com/p/47949269" target="_blank">开放域下的对话系统</a></li>
			          <li> <a href="https://zhuanlan.zhihu.com/p/61702784" target="_blank">引入知识库生成故事结尾</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/35997048" target="_blank">AAAI2018会议中的应答生成</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/32089282" target="_blank">Attention学习笔记</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/29967933" target="_blank">从文本生成看Seq2Seq模型</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/26604113" target="_blank">EMNLP 2014：利用RNN模型生成中国古代诗歌</a></li>
				 </div>
            </ul>
        </td></tr>

        <tr><td width="100%" valign="middle">
            <heading>Publications</heading> <br><br>
		
	    <heading2><i>Preprint</i></heading2><br><br>
		
	       <!--<div onmouseover="document.getElementById('KG-Coherence').style.display = 'block';"onmouseout="document.getElementById('KG-Coherence').style.display='none';">
              <a href=""><papertitle>Review Generation by Text Planning</papertitle></a><br>
              <em>Under Review</em><br>
              <a href="">pdf</a>  /  <a href="">code</a> 
            </div>
              <div id="KG-Coherence" style="display:none;text-align:justify">
              </div><br>-->
		
	      <div onmouseover="document.getElementById('WenLan').style.display = 'block';"onmouseout="document.getElementById('WenLan').style.display='none';">
              <a href="https://arxiv.org/pdf/2103.06561.pdf">
                <papertitle>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</papertitle></a><br>
	      <i>Yuqi Huo</i>, <i>Manli Zhang</i>, <i>Guangzhen Liu</i>, <i>Haoyu Lu</i>, <i>Yizhao Gao</i>, <i>Guoxing Yang</i>, <i>Jingyuan Wen</i>, 
		      <i>Heng Zhang</i>, <i>Baogui Xu</i>,
		      <i>Weihao Zheng</i>, <i>Zongzheng Xi</i>, <i>Yueqian Yang</i>, <i>Anwen Hu</i>, <i>Jinming Zhao</i>, <i>Ruichen Li</i>, 
		      <i>Yida Zhao</i>, <i>Liang Zhang</i>,
		      <i>Yuqing Song</i>, <i>Xin Hong</i>, <i>Wanqing Cui</i>, <i>Danyang Hou</i>, <i>Yingyan Li</i>, <i><strong>Junyi Li</strong></i>, 
		      <i>Peiyu Liu</i>, <i>Zheng Gong</i>, 
		      <i>Chuhao Jin</i>, <i>Yuchong Sun</i>, <i>Shizhe Chen</i>, <i>Zhiwu Lu</i>*, <i>Zhicheng Dou</i>, <i>Qin Jin</i>, 
		      <i>Yanyan Lan</i>, <i>Wayne Xin Zhao</i>, 
		      <i>Ruihua Song</i>*, <i>Ji-Rong Wen</i>*<br>
              <a href="https://arxiv.org/pdf/2103.06561.pdf">pdf</a>  /  <a href="">code</a> 
            </div>
              <div id="WenLan" style="display:none;text-align:justify">
              Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them 
		      explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation 
		      between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to 
		      implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese 
		      project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a 
		      two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that 
		      adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into 
		      the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in 
		      limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for 
		      pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER 
		      and OpenAI CLIP on various downstream tasks.
              </div><br>
		
	      <div onmouseover="document.getElementById('TextBox').style.display = 'block';"onmouseout="document.getElementById('TextBox').style.display='none';">
              <a href="https://arxiv.org/pdf/2101.02046.pdf">
                <papertitle>TextBox: A Unified, Modularized, and Extensible Framework for Text Generation</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>&dagger;,
              <i>Tianyi Tang</i>&dagger;,
	      <i>Gaole He</i>,
	      <i>Jinhao Jiang</i>,
		<i>Xiaoxuan Hu</i>,
		 <i>Puzhao Xie</i>,
              <i>Wayne Xin Zhao</i>*,
	      <i>Ji-Rong Wen</i>
              <br>
              <a href="https://arxiv.org/pdf/2101.02046.pdf">pdf</a>  /  <a href="https://github.com/RUCAIBox/TextBox">code</a> 
            </div>
              <div id="TextBox" style="display:none;text-align:justify">
              We release an open library, called TextBox, which provides a unified, modularized, and extensible text generation framework. TextBox aims 
		      to support a broad set of text generation tasks and models. In TextBox, we implements several text generation models on benchmark 
		      datasets, covering the categories of VAE, GAN, pre-trained language models, etc. Meanwhile, our library maintains sufficient 
		      modularity and extensibility by properly decomposing the model architecture, inference, learning process into highly reusable modules, 
		      which allows easily incorporating new models into our framework. It is specially suitable for researchers and practitioners to 
		      efficiently reproduce baseline models and develop new models. TextBox is implemented based on PyTorch, and released under 
		      Apache License 2.0 at the link https://github.com/RUCAIBox/TextBox.
              </div><br>
		
	    <heading2><i>2020</i></heading2><br><br>
		
	      <div onmouseover="document.getElementById('KG-Review').style.display = 'block';"onmouseout="document.getElementById('KG-Review').style.display='none';">
              <a href="https://arxiv.org/pdf/2010.01480.pdf">
                <papertitle>Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Siqing Li</i>,
              <i>Wayne Xin Zhao</i>*,
              <i>Gaole He</i>,
		<i>Zhicheng Wei</i>,
		 <i>Nicholas Jing Yuan</i>,
	      <i>Ji-Rong Wen</i>
              <br>
              <em>29th ACM International Conference on Information and Knowledge Management (CIKM)</em>, 2020 <br>
              <a href="https://arxiv.org/pdf/2010.01480.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/CapsGNN-Review-Generation">code</a> 
            </div>
              <div id="KG-Review" style="display:none;text-align:justify">
              Personalized review generation (PRG) aims to automatically produce review text reflecting user preference, which is a challenging
		natural language generation task. Most of previous studies do not
		explicitly model factual description of products, tending to generate
		uninformative content. Moreover, they mainly focus on word-level
		generation, but cannot accurately reflect more abstractive user
		preference in multiple aspects. <br>
		&emsp;To address the above issues, we propose a novel knowledgeenhanced PRG model based on capsule graph neural network (CapsGNN). We first construct a heterogeneous knowledge graph (HKG)
		for utilizing rich item attributes. We adopt Caps-GNN to learn
		graph capsules for encoding underlying characteristics from the
		HKG. Our generation process contains two major steps, namely
		aspect sequence generation and sentence generation. First, based
		on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence. Then, conditioned on the inferred aspect
		label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To
		our knowledge, we are the first to utilize knowledge graph for the
		PRG task. The incorporated KG information is able to enhance user
		preference at both aspect and word levels. Extensive experiments
		on three real-world datasets have demonstrated the effectiveness
		of our model on the PRG task.

              </div><br>
			
	      <div onmouseover="document.getElementById('GAN-KGC').style.display = 'block';"onmouseout="document.getElementById('GAN-KGC').style.display='none';">
              <a href="https://arxiv.org/pdf/2003.12718.pdf">
                <papertitle>Mining Implicit Entity Preference from User-Item Interaction Data for Knowledge Graph Completion via Adversarial Learning</papertitle></a><br>
              <i>Gaole He</i>,
              <i><strong>Junyi Li</strong></i>,
              <i>Wayne Xin Zhao</i>*,
              <i>Peiju Liu</i>,
			  <i>Ji-Rong Wen</i>
              <br>
              <em>International World Wide Web Conference (WWW)</em>, 2020 <br>
              <a href="https://arxiv.org/pdf/2003.12718.pdf">pdf</a>  /  <a href="https://github.com/RUCAIBox/UPGAN">code</a> 
            </div>
              <div id="GAN-KGC" style="display:none;text-align:justify">
              The task of Knowledge Graph Completion (KGC) aims to automatically infer the missing fact information in Knowledge Graph (KG).In this paper, we take a new perspective that aims to leverage rich
				user-item interaction data (user interaction data for short) for improving the KGC task. Our work is inspired by the observation that
				many KG entities correspond to online items in application systems.
				However, the two kinds of data sources have very different intrinsic
				characteristics, and it is likely to hurt the original representation
				performance using simple fusion strategy. <br>
				&emsp;To address this challenge, we propose a novel adversarial learning approach for leveraging user interaction data for the KGC task.
				Our generator is isolated from user interaction data, and improves
				itself according to the feedback from the discriminator. The discriminator takes the learned useful information from user interaction
				data as input, and gradually enhances the evaluation capacity in
				order to identify the fake samples generated by the generator. To
				discover implicit entity preference of users, we design an elaborate
				collaborative learning algorithms based on graph neural networks,
				which will be jointly optimized with the discriminator. Such an
				approach is effective to alleviate the issues about data heterogeneity
				and semantic complexity for the KGC task. Extensive experiments
				on three real-world datasets have demonstrated the effectiveness
				of our approach on the KGC task.

              </div><br>
		

            <heading2><i>2019</i></heading2><br><br>

            <div onmouseover="document.getElementById('ACF').style.display = 'block';"
                onmouseout="document.getElementById('ACF').style.display='none';">
              <a href="https://www.aclweb.org/anthology/P19-1190.pdf">
                <papertitle>Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Wayne Xin Zhao</i>*,
			  <i>Ji-Rong Wen</i>,
			  <i>Yang Song</i>
              <br>
              <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2019 <br>
              <a href="https://www.aclweb.org/anthology/P19-1190.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/Coarse-to-Fine-Review-Generation">code</a> 
            </div>
              <div id="ACF" style="display:none;text-align:justify">
              Generating long and informative review text
				is a challenging natural language generation
				task. Previous work focuses on word-level
				generation, neglecting the importance of topical and syntactic characteristics from natural
				languages. In this paper, we propose a novel
				review generation model by characterizing an
				elaborately designed aspect-aware coarse-tofine generation process. First, we model the
				aspect transitions to capture the overall content
				flow. Then, to generate a sentence, an aspectaware sketch will be predicted using an aspectaware decoder. Finally, another decoder fills in
				the semantic slots by generating corresponding words. Our approach is able to jointly
				utilize aspect semantics, syntactic sketch, and
				context information. Extensive experiments
				results have demonstrated the effectiveness of
				the proposed model.
              </div><br>
              * Corresponding author <br>
	      &dagger; Equal contribution
        </td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Open Source Projects</heading> <br><br>
		(Most of my research work are open-source. Here are some my preferable projects!)
		<ul>
		<li><a href="https://github.com/RUCAIBox/TextBox" target="_black">TextBox</a><br>A unified, comprehensive and efficient framework for reproducing and developing 
			text generation algorithms, covering more than 20 base models and nearly 10 benchmarks.</li>
		</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Professional Services</heading> <br><br>
			<ul>
			<li>Reviewer
				<ul>
					<li>Journal: TALLIP</li>
					<li>Conference: AAAI 2021, IJCAI 2021, KDD 2021</li>
				</ul>
			</li>
			<li>Chair
				<ul>
					<li>CSSNLP 2020 (Co-Chair)</li>
				</ul>
			</li>
			</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Selected Awards and Honors</heading> <br><br>
			<ul>
			<li>SIGIR Student Travel Grant (CIKM 2020)</li>
			<li>National Scholarship for Graduate Student (top 2% students), Ministry of Education of P.R.China, 2019</li>
			<li>China Undergraduate Mathematical Contest in Modeling, Second Prize in Beijing Contest District, 2016</li>	
			</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Education</heading> <br><br>
			<ul>
			<li>Ph.D. student of Artificial Intelligence, Renmin University of China, 2020-present
			</li>
			<li>M.Sc. of Computer Application, Renmin University of China, 2018-2020
			</li>
			<li>B.Sc. of Computer Science, Renmin University of China, 2014-2018
			</li>
			</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Miscellaneous</heading> <br><br>
		<p>
		  <a href="https://en.wikipedia.org/wiki/Liu_Haoran">Who?</a> My idol. Haoran brings me happiness during my life. He just likes "fill in the blanks of my life".
		</p>
			
	</td></tr><br>
		
	<tr><td width="100%" valign="middle">
	<center>
	<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=571ds4qroi6&amp;s=220&amp;m=7&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
	</center>
	</td></tr>
		
	</tbody></table>
	   
	
	<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-59618557-1', 'auto');
                ga('send', 'pageview');

              </script>

    </td>
    </tr>
  </tbody></table>
  
  <!--footer start-->
 <footer class="footer">
	<p>Copyright 2021. All Rights Reserved by Junyi Li.</p>
 </footer>
     <!--footer end-->
  
</body></html>
