<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html manifest="welcome.manifest">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta content="yes" name="apple-mobile-web-app-capable">
  <meta content="black" name="apple-mobile-web-app-status-bar-style">
  <meta content="telephone=no" name="format-detection">
  <meta name="author" content="Junyi Li">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #07889b; /*#1772d0;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #e37222; /*#f7b733;*/ /*f09228;*/
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    strong {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    heading {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 22px;
    color: #e37222; /*#fc4a1a;*/
    }
    heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 18px;
    }
    papertitle {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    font-weight: 700;
    }
    name {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link href="https://tuchuang-1258543525.cos.ap-beijing.myqcloud.com/20180614_161325021_iOS.jpg" rel="Shortcut Icon" type="image/x-icon">
  <title>Junyi Li (李军毅)</title>

  <link href="./stylesheets/main.css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
/*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

</head>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Junyi Li (李军毅)</name><br>
        </p>
        <p style="text-align:justify">
		I am a Ph.D. student jointly supervised by <a href="http://rali.iro.umontreal.ca/nie-site/jian-yun-nie-en/">Prof. Jian-Yun Nie</a> from <a href="https://diro.umontreal.ca/accueil/">DIRO</a>, 
		<a href="https://www.umontreal.ca/en/">Universite de Montreal</a> and <a href="http://playbigdata.ruc.edu.cn/batmanfly/">Prof. Xin Zhao</a> from <a href="http://ai.ruc.edu.cn/english/index.htm">GSAI</a>, 
		<a href="https://ruc.edu.cn">Renmin University of China</a>. I have a broad interest in natural language processing, with an emphasis on natural language generation (NLG), especially based on pretrained language models (PLMs).
	</p>
	<p>Email: junyi.li at umontreal dot ca / lijunyi at ruc dot edu dot cn</p>
<!--         <p>
        <i>There is no authority in science. No one can tell whether your research matters or not. Or how much it matters. All you can do is to contribute to human knowledge and hope it will matter. Even if it doesn't, it does. It eliminates an idea.</i><br>
        </p>
		<p align="right">
		<i>-- Rich Sutton</i>
		</p> -->


        <p align="center">
			<a href="./images/resume.pdf" target="_blank">CV</a> &nbsp;/&nbsp;
			<a href="https://scholar.google.com/citations?user=zeWrn-4AAAAJ&hl=zh-CN" target="_blank">Google Scholar</a> &nbsp;/&nbsp;
			<a href="https://github.com/turboLJY" target="_blank"> GitHub </a> &nbsp;/&nbsp;
		    <a href="https://www.zhihu.com/people/li-jun-yi-93" target="_blank">Zhihu</a>
        </p>
        </td>
        <td width="33%">
        <img src="./images/lijunyi.jpg" width="95%">
        </td>
      </tr>
  </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>

        <tr><td>
            <heading>Zhihu Posts</heading>
            <ul>
	      <li> <a href="https://zhuanlan.zhihu.com/p/353972216" target="_blank">Knowledge-Enhanced Text Generation: 知识增强的文本生成研究进展</a></li>
              <li> <a href="https://zhuanlan.zhihu.com/p/69069509" target="_blank">ACL 2019 | 渐入佳境，基于主题感知的Coarse-to-Fine机制的在线评论生成</a></li>
			  <li> <a href="https://zhuanlan.zhihu.com/p/36880287" target="_blank">GAN+文本生成：让文本以假乱真</a></li>
			  
              <li> <a href="javascript:toggle_vis('zhihu-blogs')" style="color:black">show more</a> </li>
				  <div id="zhihu-blogs" style="display:none"> 
		   		  <li> <a href="https://zhuanlan.zhihu.com/p/47949269" target="_blank">开放域下的对话系统</a></li>
			          <li> <a href="https://zhuanlan.zhihu.com/p/61702784" target="_blank">引入知识库生成故事结尾</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/35997048" target="_blank">AAAI2018会议中的应答生成</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/32089282" target="_blank">Attention学习笔记</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/29967933" target="_blank">从文本生成看Seq2Seq模型</a></li>
				  <li> <a href="https://zhuanlan.zhihu.com/p/26604113" target="_blank">EMNLP 2014：利用RNN模型生成中国古代诗歌</a></li>
				 </div>
            </ul>
        </td></tr>

        <tr><td width="100%" valign="middle">
            <heading>Publications</heading> <br><br>
		
	    <heading2><i>Preprint</i></heading2><br><br>
		
	      <div onmouseover="document.getElementById('MVP').style.display = 'block';"onmouseout="document.getElementById('MVP').style.display='none';">
              <a href="">
                <papertitle>MVP: Multi-task Supervised Pre-training for Natural Language Generation</papertitle></a><br>
              <i>Tianyi Tang</i>,
	      <i><strong>Junyi Li</strong></i>,
	      <i>Wayne Xin Zhao</i>*,
              <i>Ji-Rong Wen</i>
              <br>
	      <a href="https://arxiv.org/pdf/2206.12131.pdf">pdf</a>  /  <a href="https://github.com/RUCAIBox/MVP">code</a>
            </div>
              <div id="MVP" style="display:none;text-align:justify">
              	Pre-trained language models (PLMs) have achieved notable success in natural language generation (NLG) tasks. Up to now, most of 
		      the PLMs are pre-trained in an unsupervised manner using large-scale general corpus. In the meanwhile, an increasing number 
		      of models pre-trained with less labeled data showcase superior performance compared to unsupervised models. Motivated by the 
		      success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. 
		      For pre-training the text generation model MVP, we collect a labeled pre-training corpus from 45 datasets over seven generation 
		      tasks. For each task, we further pre-train specific soft prompts to stimulate the model capacity in performing a specific task. 
		      Extensive experiments have demonstrated the effectiveness of our supervised pre-training in a number of NLG tasks, and our 
		      general methods achieve state-of-the-art performance on 12 of 17 datasets.
              </div><br>
		
	      <div onmouseover="document.getElementById('PLM4TG-Survey').style.display = 'block';"onmouseout="document.getElementById('PLM4TG-Survey').style.display='none';">
              <a href="https://arxiv.org/pdf/2201.05273.pdf">
                <papertitle>A Survey of Pretrained Language Models Based Text Generation</papertitle></a><br>
	      <i><strong>Junyi Li</strong></i>&dagger;, <i>Tianyi Tang</i>&dagger;, <i>Wayne Xin Zhao</i>*, 
		      <i>Jian-Yun Nie</i>, <i>Ji-Rong Wen</i><br>
              <a href="https://arxiv.org/pdf/2201.05273.pdf">pdf</a> 
            </div>
              <div id="PLM4TG-Survey" style="display:none;text-align:justify">
              Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly 
		      advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). Grounding text generation 
		      on PLMs is seen as a promising direction in both academia and industry. In this survey, we present the recent advances achieved in the 
		      topic of PLMs for text generation. In detail, we begin with introducing three key points of applying PLMs to text generation: 1) how to 
		      encode the input data as representations preserving input semantics which can be fused into PLMs; 2) how to design a universal and 
		      performant architecture of PLMs served as generation models; and 3) how to optimize PLMs given the reference text and ensure the generated 
		      text satisfying special text properties. Then, we figure out several challenges and future directions within each key point. Next, 
		      we present a summary of various useful resources and typical text generation applications to work with PLMs. Finally, we conclude and 
		      summarize the contribution of this survey.
              </div><br>
		
	      <div onmouseover="document.getElementById('WenLan').style.display = 'block';"onmouseout="document.getElementById('WenLan').style.display='none';">
              <a href="https://arxiv.org/pdf/2103.06561.pdf">
                <papertitle>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</papertitle></a><br>
	      <i>Yuqi Huo</i>, <i>Manli Zhang</i>, <i>Guangzhen Liu</i>, <i>Haoyu Lu</i>, <i>Yizhao Gao</i>, <i>Guoxing Yang</i>, <i>Jingyuan Wen</i>, 
		      <i>Heng Zhang</i>, <i>Baogui Xu</i>,
		      <i>Weihao Zheng</i>, <i>Zongzheng Xi</i>, <i>Yueqian Yang</i>, <i>Anwen Hu</i>, <i>Jinming Zhao</i>, <i>Ruichen Li</i>, 
		      <i>Yida Zhao</i>, <i>Liang Zhang</i>,
		      <i>Yuqing Song</i>, <i>Xin Hong</i>, <i>Wanqing Cui</i>, <i>Danyang Hou</i>, <i>Yingyan Li</i>, <i><strong>Junyi Li</strong></i>, 
		      <i>Peiyu Liu</i>, <i>Zheng Gong</i>, 
		      <i>Chuhao Jin</i>, <i>Yuchong Sun</i>, <i>Shizhe Chen</i>, <i>Zhiwu Lu</i>*, <i>Zhicheng Dou</i>, <i>Qin Jin</i>, 
		      <i>Yanyan Lan</i>, <i>Wayne Xin Zhao</i>, 
		      <i>Ruihua Song</i>*, <i>Ji-Rong Wen</i>*<br>
              <a href="https://arxiv.org/pdf/2103.06561.pdf">pdf</a>  /  <a href="">code</a> 
            </div>
              <div id="WenLan" style="display:none;text-align:justify">
              Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them 
		      explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation 
		      between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to 
		      implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese 
		      project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a 
		      two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that 
		      adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into 
		      the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in 
		      limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for 
		      pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER 
		      and OpenAI CLIP on various downstream tasks.
              </div><br>
		
	    <heading2><i>2022</i></heading2><br><br>
		
	    <div onmouseover="document.getElementById('Elmer').style.display = 'block';"onmouseout="document.getElementById('Elmer').style.display='none';">
              <a href="">
                <papertitle>ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
	      <i>Tianyi Tang</i>,
	      <i>Wayne Xin Zhao</i>*,
		    <i>Jian-Yun Nie</i>,
	      <i>Ji-Rong Wen</i>
              <br>
	      <em>The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022<br>
              <a href="">pdf</a>  /  <a href="">code</a> 
            </div>
              <div id="Elmer" style="display:none;text-align:justify">
              We consider the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) 
		      paradigm is adopted for generating texts in a token-by-token manner.  Despite many advantages of AR generation, it has been 
		      widely blamed for its inefficient inference. Therefore, non-autoregressive (NAR) models are proposed to generate all target 
		      tokens simultaneously. However, NAR models usually generate texts of lower quality due to the absence of token dependency 
		      in the output text. In this paper, we propose ELMER: an efficient and effective PLM for NAR text generation to explicitly 
		      model the token dependency during NAR generation. By leveraging the early exit technique, ELMER enables the token generations 
		      at different layers, according to their prediction confidence (a more confident token will exit at a lower layer). Besides, 
		      we propose a novel Layer Permutation Language Modeling to pre-train ELMER by permuting the exit layer for each 
		      token in sequences. Experiments on three text generation tasks show that ELMER significantly outperforms NAR models and 
		      further narrows the performance gap with AR PLMs (\eg ELMER (29.92) vs BART (30.61) ROUGE-L in XSUM) while achieving 
		      over 10x inference speedups.
              </div><br>
		
	    <div onmouseover="document.getElementById('TextBox2.0').style.display = 'block';"onmouseout="document.getElementById('TextBox2.0').style.display='none';">
              <a href="">
                <papertitle>TextBox 2.0: A Text Generation Library with Pre-trained Language Models</papertitle></a><br>
              <i>Tianyi Tang</i>&dagger;,
              <i><strong>Junyi Li</strong></i>&dagger;,
	      <i>Zhipeng Chen</i>&dagger;,
	      <i>Yiwen Hu</i>,
		<i>Zhuohao Yu</i>,
		 <i>Wenxun Dai</i>,
		 <i>Wayne Xin Zhao</i>*,
		    <i>Jian-Yun Nie</i>,
	      <i>Ji-Rong Wen</i>
              <br>
	      <em>The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022, System Demonstration <br>
              <a href="">pdf</a>  /  <a href="https://github.com/RUCAIBox/TextBox">code</a> 
            </div>
              <div id="TextBox2.0" style="display:none;text-align:justify">
              To facilitate research on text generation, this paper presents a comprehensive, unified, and standardized library, TextBox 2.0, 
		      focusing on the use of pre-trained language models (PLMs). To be comprehensive, our library considers 13 common text 
		      generation tasks and their corresponding 83 datasets and incorporates 36 PLMs covering general, translation, dialogue, 
		      controllable, distilled, Chinese, and lightweight PLMs. We also implement 4 efficient training strategies and provide 4
		      generation objectives for pre-training new PLMs from scratch. To be unified and standardized, we carefully design the 
		      interfaces along the research pipeline (from data loading to training and evaluation), ensuring that each step can be 
		      conducted in a unified, standard way. Though comprehensive and powerful, the use of our library is rather simple, 
		      either by friendly Python API or command line. Besides, we perform extensive experiments to validate the effectiveness 
		      of our library and provide useful methods to analyze the generated results. 
              </div><br>
		
	    <div onmouseover="document.getElementById('Context-Tuning').style.display = 'block';"onmouseout="document.getElementById('Context-Tuning').style.display='none';">
              <a href="">
                <papertitle>Context-Tuning: Learning Contextualized Prompts for Natural Language Generation</papertitle></a><br>
              <i>Tianyi Tang</i>,
	      <i><strong>Junyi Li</strong></i>,
	      <i>Wayne Xin Zhao</i>*,
              <i>Ji-Rong Wen</i>
              <br>
	      <em>The 29th International Conference on Computational Linguistics (COLING)</em>, 2022<br>
              <a href="">pdf</a>  /  <a href="">code</a>
            </div>
              <div id="Context-Tuning" style="display:none;text-align:justify">
              	Recently, pretrained language models (PLMs) have made exceptional success in language generation. To leverage the rich knowledge encoded 
		      by PLMs, a simple yet powerful mechanism is to use prompts, in the form of either discrete tokens or continuous embeddings. 
		      In existing studies, manual prompts are time-consuming and require domain expertise, while continuous prompts are typically 
		      independent of the inputs. To address this issue, we propose a novel continuous prompting approach, called Context-Tuning, 
		      to fine-tuning PLMs for natural language generation. Firstly, the prompts are derived based on the input text, so that they 
		      can elicit useful knowledge from PLMs for generation. We refer to such prompts as contextualized prompts. Secondly, to further 
		      enhance the relevance of the generated text to the inputs, we utilize continuous inverse prompting to refine the process of 
		      natural language generation by modeling an inverse generation process from output to input. Moreover, we propose a lightweight 
		      context-tuning, fine-tuning only 0.4% of parameters while retaining well performance.
              </div><br>
		
	    <div onmouseover="document.getElementById('VL-Survey').style.display = 'block';"onmouseout="document.getElementById('VL-Survey').style.display='none';">
              <a href="https://arxiv.org/pdf/2202.10936.pdf">
                <papertitle>A Survey of Vision-Language Pre-Trained Models</papertitle></a><br>
              <i>Yifan Du</i>&dagger;,
	      <i>Zikang Liu</i>&dagger;,
	      <i><strong>Junyi Li</strong></i>,
	      <i>Wayne Xin Zhao</i>*
              <br>
	      <em>The 31th International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2022, Survey Track <br>
              <a href="https://arxiv.org/pdf/2202.10936.pdf">pdf</a> 
            </div>
              <div id="VL-Survey" style="display:none;text-align:justify">
              	As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream
		      techniques in natural language processing~(NLP) and computer vision~(CV). How to adapt pre-training to the field of 
		      Vision-and-Language~(V-L) learning and improve downstream task performance becomes a focus of multimodal learning. 
		      In this paper, we review the recent progress in Vision-Language Pre-Trained Models~(VL-PTMs). As the core content, 
		      we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. 
		      Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image 
		      representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. 
		      We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with 
		      synthesis and pointer to related research.
              </div><br>
		
            <div onmouseover="document.getElementById('ElitePLM').style.display = 'block';"onmouseout="document.getElementById('ElitePLM').style.display='none';">
              <a href="https://arxiv.org/pdf/2205.01523.pdf">
                <papertitle>ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
	      <i>Tianyi Tang</i>,
	      <i>Zheng Gong</i>,
		    <i>Lixin Yang</i>,
		    <i>Zhuohao Yu</i>,
		    <i>Zhipeng Chen</i>,
		    <i>Jingyuan Wang</i>,
		<i>Wayne Xin Zhao</i>*,
		    <i>Ji-Rong Wen</i>
              <br>
	      <em>The North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2022<br>
              <a href="https://arxiv.org/pdf/2205.01523.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/ElitePLM">code</a> 
            </div>
              <div id="ElitePLM" style="display:none;text-align:justify">
              	Pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on 
		      systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study on 
		      general language ability evaluation of PLMs (ElitePLM). In our study, we design four evaluation dimensions, i.e., memory, 
		      comprehension, reasoning, and composition, to measure ten widely-used PLMs within five categories. Our empirical results 
		      demonstrate that: (1) PLMs with varying training objectives and strategies are good at different ability tests; (2) fine-tuning 
		      PLMs in downstream tasks is usually sensitive to the data size and distribution; (3) PLMs have excellent transferability between 
		      similar tasks. Moreover, our final predicted results of PLMs can be reused as an open resource for more depth and granularity in 
		      analyzing PLMs' language abilities. This paper can guide the future work to choose, apply, and design PLMs for specific tasks.
              </div><br>
		
	    <div onmouseover="document.getElementById('PTG').style.display = 'block';"onmouseout="document.getElementById('PTG').style.display='none';">
              <a href="https://arxiv.org/pdf/2205.01543.pdf">
                <papertitle>Learning to Transfer Prompts for Text Generation</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
	      <i>Tianyi Tang</i>,
	      <i>Jian-Yun Nie</i>,
	      <i>Ji-Rong Wen</i>,
		<i>Wayne Xin Zhao</i>*
              <br>
	      <em>The North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2022<br>
              <a href="https://arxiv.org/pdf/2205.01543.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/Transfer-Prompts-for-Text-Generation">code</a> 
            </div>
              <div id="PTG" style="display:none;text-align:justify">
              	Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. However, it is difficult 
		      to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that 
		      can adapt to various text generation tasks based on PLMs. Prompt-based learning offers a potential solution. There are two 
		      major challenges for applying prompt-based methods to data-scarce text generation tasks in a transferable setting. First, 
		      it is difficult to effectively transfer prompts for new tasks. Second, it is important to design effective transferring 
		      strategy considering both task- and instance-level information. To address these issues, we propose a novel prompt-based 
		      transfer learning approach for text generation called PTG. PTG learns a set of source prompts for various source generation 
		      tasks and then transfers these prompts to perform target generation tasks through an adaptive attention mechanism 
		      considering both task- and instance-level information. In extensive experiments, PTG yields competitive or better 
		      results than fine-tuning methods. We will release our source prompts as an open-source library, which can be added or reused 
		      to improve new generation tasks for future researches.
              </div><br>
		
	    <heading2><i>2021</i></heading2><br><br>
		
	    <div onmouseover="document.getElementById('ThreeGAN').style.display = 'block';"onmouseout="document.getElementById('ThreeGAN').style.display='none';">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-85899-5_4">
                <papertitle>Generating Long and Coherent Text with Multi-Level Generative Adversarial Networks</papertitle></a><br>
              <i>Tianyi Tang</i>,
              <i><strong>Junyi Li</strong></i>,
	      <i>Wayne Xin Zhao</i>*,
	      <i>Ji-Rong Wen</i>
              <br>
	      <em>The 5th APWeb-WAIM International Joint Conference on Web and Big Data (APWeb-WAIM)</em>, 2021<br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-85899-5_4">pdf</a>  /  <a href="">code</a> 
            </div>
              <div id="ThreeGAN" style="display:none;text-align:justify">
              	In this paper, we study the task of generating long and coherent text. In the literature, Generative Adversarial Nets (GAN) based methods 
		      have been one of the mainstream approaches to generic text generation. We aim to improve two aspects of GAN-based methods in generic 
		      text generation, namely long sequence optimization and semantic coherence enhancement. For this purpose, we propose a novel Multi-Level 
		      Generative Adversarial Networks (MLGAN) for long and coherent text generation. Our approach explicitly models the text generation 
		      process at three different levels, namely paragraph-, sentence- and word-level generation. At the top two levels, we generate 
		      continuous paragraph vectors and sentence vectors as \emph{semantic sketches} to plan the entire content. While, at the bottom 
		      level we generate discrete word tokens for realizing the sentences. Furthermore, we utilize a Conditional GAN architecture to 
		      enhance the inter-sentence coherence by injecting paragraph vectors for sentence vector generation. Extensive experiments results 
		      have demonstrated the effectiveness of the proposed model.
              </div><br>
		
	    <div onmouseover="document.getElementById('TextBox').style.display = 'block';"onmouseout="document.getElementById('TextBox').style.display='none';">
              <a href="https://arxiv.org/pdf/2101.02046.pdf">
                <papertitle>TextBox: A Unified, Modularized, and Extensible Framework for Text Generation</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>&dagger;,
              <i>Tianyi Tang</i>&dagger;,
	      <i>Gaole He</i>,
	      <i>Jinhao Jiang</i>,
		<i>Xiaoxuan Hu</i>,
		 <i>Puzhao Xie</i>,
		 <i>Zhipeng Chen</i>, 
		 <i>Zhuohao Yu</i>,
              <i>Wayne Xin Zhao</i>*,
	      <i>Ji-Rong Wen</i>
              <br>
	      <em>The 59th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2021, System Demonstration <br>
              <a href="https://arxiv.org/pdf/2101.02046.pdf">pdf</a>  /  <a href="https://github.com/RUCAIBox/TextBox">code</a> 
            </div>
              <div id="TextBox" style="display:none;text-align:justify">
              We release an open library, called TextBox, which provides a unified, modularized, and extensible text generation framework. TextBox aims 
		      to support a broad set of text generation tasks and models. In TextBox, we implements several text generation models on benchmark 
		      datasets, covering the categories of VAE, GAN, pre-trained language models, etc. Meanwhile, our library maintains sufficient 
		      modularity and extensibility by properly decomposing the model architecture, inference, learning process into highly reusable modules, 
		      which allows easily incorporating new models into our framework. It is specially suitable for researchers and practitioners to 
		      efficiently reproduce baseline models and develop new models. TextBox is implemented based on PyTorch, and released under 
		      Apache License 2.0 at the link https://github.com/RUCAIBox/TextBox.
              </div><br>
		
	    <div onmouseover="document.getElementById('KG-to-Text').style.display = 'block';"onmouseout="document.getElementById('KG-to-Text').style.display='none';">
              <a href="https://aclanthology.org/2021.findings-acl.136.pdf">
                <papertitle>Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Tianyi Tang</i>,
	      <i>Wayne Xin Zhao</i>*,
	      <i>Ji-Rong Wen</i>
              <br>
		    <em>Findings of The 59th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2021 <br>
              <a href="https://aclanthology.org/2021.findings-acl.136.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/Few-Shot-KG2Text">code</a> 
            </div>
              <div id="KG-to-Text" style="display:none;text-align:justify">
              This paper studies how to automatically generate a natural language text that describes facts in knowledge graph (KG). 
		      Considering a fewshot setting, we leverage the excellent capacities of pretrained language models (PLMs) in
		      language understanding and generation. We introduce three major technical contributions, namely representation 
		      alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for 
		      deriving better input representations, and multitask learning for learning the correspondence between KG and text. 
		      Extensive experiments on three benchmarks have demonstrated the effectiveness of our model on KG-to-text generation task. 
		      In particular, our model outperforms existing systems on most few-shot settings.
              </div><br>
		
	    <div onmouseover="document.getElementById('PLM-Survey').style.display = 'block';"onmouseout="document.getElementById('PLM-Survey').style.display='none';">
              <a href="https://arxiv.org/pdf/2105.10311.pdf">
                <papertitle>Pretrained Language Model for Text Generation: A Survey</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>&dagger;,
              <i>Tianyi Tang</i>&dagger;,
	      <i>Wayne Xin Zhao</i>*,
	      <i>Ji-Rong Wen</i>
              <br>
		    <em>The 30th International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2021, Survey Track <br>
              <a href="https://arxiv.org/pdf/2105.10311.pdf">pdf</a>   
            </div>
              <div id="PLM-Survey" style="display:none;text-align:justify">
              Text generation has become one of the most important yet challenging tasks in natural language
		processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper,
		we presents an overview of the major advances
		achieved in the topic of PLM for text generation.
		As the preliminaries, we present the general task
		definition and briefly describe the mainstream architectures of PLMs. As the core content, we discuss how to adapt existing PLMs to model different
		input data and satisfy the properties in the generated text. We further summarize several important
		fine-tuning strategies for text generation. Finally,
		we present several future directions and conclude
		this paper. Our survey aims to provide text generation researchers a synthesis and pointer to relatedresearch.
              </div><br>
		
	    <div onmouseover="document.getElementById('KG-Coherence').style.display = 'block';"onmouseout="document.getElementById('KG-Coherence').style.display='none';">
              <a href="https://arxiv.org/pdf/2105.03815.pdf">
		      <papertitle>Knowledge-based Review Generation by Coherence Enhanced Text Planning</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Wayne Xin Zhao</i>*,
              <i>Zhicheng Wei</i>,
	      <i>Nicholas Jing Yuan</i>,
	      <i>Ji-Rong Wen</i>
              <br>
		<em>The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</em>, 2021 <br>
              <a href="https://arxiv.org/pdf/2105.03815.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/Coherence-Review-Generation">code</a> 
            </div>
              <div id="KG-Coherence" style="display:none;text-align:justify">
		  As a natural language generation task, it is challenging to generate informative and coherent review text. In order to enhance
			the informativeness of the generated text, existing solutions typically learn to copy entities or triples from knowledge graphs (KGs).
			However, they lack overall consideration to select and arrange the
			incorporated knowledge, which tends to cause text incoherence. <br>
		&emsp;To address the above issue, we focus on improving entity-centric
			coherence of the generated reviews by leveraging the semantic structure of KGs. In this paper, we propose a novel Coherence Enhanced
			Text Planning model (CETP) based on knowledge graphs (KGs) to
			improve both global and local coherence for review generation. The
			proposed model learns a two-level text plan for generating a document: (1) the document plan is modeled as a sequence of sentence
			plans in order, and (2) the sentence plan is modeled as an entitybased subgraph from KG. Local coherence can be naturally enforced
			by KG subgraphs through intra-sentence correlations between entities. For global coherence, we design a hierarchical self-attentive
			architecture with both subgraph- and node-level attention to enhance the correlations between subgraphs. To our knowledge, we
			are the first to utilize a KG-based text planning model to enhance
			text coherence for review generation. Extensive experiments on
			three datasets confirm the effectiveness of our model on improving
			the content coherence of generated texts.
              </div><br>
		
	    <heading2><i>2020</i></heading2><br><br>
		
	      <div onmouseover="document.getElementById('KG-Review').style.display = 'block';"onmouseout="document.getElementById('KG-Review').style.display='none';">
              <a href="https://arxiv.org/pdf/2010.01480.pdf">
                <papertitle>Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Siqing Li</i>,
              <i>Wayne Xin Zhao</i>*,
              <i>Gaole He</i>,
		<i>Zhicheng Wei</i>,
		 <i>Nicholas Jing Yuan</i>,
	      <i>Ji-Rong Wen</i>
              <br>
              <em>The 29th ACM International Conference on Information and Knowledge Management (CIKM)</em>, 2020 <br>
              <a href="https://arxiv.org/pdf/2010.01480.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/CapsGNN-Review-Generation">code</a> 
            </div>
              <div id="KG-Review" style="display:none;text-align:justify">
              Personalized review generation (PRG) aims to automatically produce review text reflecting user preference, which is a challenging
		natural language generation task. Most of previous studies do not
		explicitly model factual description of products, tending to generate
		uninformative content. Moreover, they mainly focus on word-level
		generation, but cannot accurately reflect more abstractive user
		preference in multiple aspects. <br>
		&emsp;To address the above issues, we propose a novel knowledgeenhanced PRG model based on capsule graph neural network (CapsGNN). We first construct a heterogeneous knowledge graph (HKG)
		for utilizing rich item attributes. We adopt Caps-GNN to learn
		graph capsules for encoding underlying characteristics from the
		HKG. Our generation process contains two major steps, namely
		aspect sequence generation and sentence generation. First, based
		on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence. Then, conditioned on the inferred aspect
		label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To
		our knowledge, we are the first to utilize knowledge graph for the
		PRG task. The incorporated KG information is able to enhance user
		preference at both aspect and word levels. Extensive experiments
		on three real-world datasets have demonstrated the effectiveness
		of our model on the PRG task.

              </div><br>
			
	      <div onmouseover="document.getElementById('GAN-KGC').style.display = 'block';"onmouseout="document.getElementById('GAN-KGC').style.display='none';">
              <a href="https://arxiv.org/pdf/2003.12718.pdf">
                <papertitle>Mining Implicit Entity Preference from User-Item Interaction Data for Knowledge Graph Completion via Adversarial Learning</papertitle></a><br>
              <i>Gaole He</i>,
              <i><strong>Junyi Li</strong></i>,
              <i>Wayne Xin Zhao</i>*,
              <i>Peiju Liu</i>,
			  <i>Ji-Rong Wen</i>
              <br>
              <em>International World Wide Web Conference (WWW)</em>, 2020 <br>
              <a href="https://arxiv.org/pdf/2003.12718.pdf">pdf</a>  /  <a href="https://github.com/RUCAIBox/UPGAN">code</a> 
            </div>
              <div id="GAN-KGC" style="display:none;text-align:justify">
              The task of Knowledge Graph Completion (KGC) aims to automatically infer the missing fact information in Knowledge Graph (KG).In this paper, we take a new perspective that aims to leverage rich
				user-item interaction data (user interaction data for short) for improving the KGC task. Our work is inspired by the observation that
				many KG entities correspond to online items in application systems.
				However, the two kinds of data sources have very different intrinsic
				characteristics, and it is likely to hurt the original representation
				performance using simple fusion strategy. <br>
				&emsp;To address this challenge, we propose a novel adversarial learning approach for leveraging user interaction data for the KGC task.
				Our generator is isolated from user interaction data, and improves
				itself according to the feedback from the discriminator. The discriminator takes the learned useful information from user interaction
				data as input, and gradually enhances the evaluation capacity in
				order to identify the fake samples generated by the generator. To
				discover implicit entity preference of users, we design an elaborate
				collaborative learning algorithms based on graph neural networks,
				which will be jointly optimized with the discriminator. Such an
				approach is effective to alleviate the issues about data heterogeneity
				and semantic complexity for the KGC task. Extensive experiments
				on three real-world datasets have demonstrated the effectiveness
				of our approach on the KGC task.

              </div><br>
		

            <heading2><i>2019</i></heading2><br><br>

            <div onmouseover="document.getElementById('ACF').style.display = 'block';"
                onmouseout="document.getElementById('ACF').style.display='none';">
              <a href="https://www.aclweb.org/anthology/P19-1190.pdf">
                <papertitle>Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Wayne Xin Zhao</i>*,
			  <i>Ji-Rong Wen</i>,
			  <i>Yang Song</i>
              <br>
              <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2019 <br>
              <a href="https://www.aclweb.org/anthology/P19-1190.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/Coarse-to-Fine-Review-Generation">code</a> 
            </div>
              <div id="ACF" style="display:none;text-align:justify">
              Generating long and informative review text
				is a challenging natural language generation
				task. Previous work focuses on word-level
				generation, neglecting the importance of topical and syntactic characteristics from natural
				languages. In this paper, we propose a novel
				review generation model by characterizing an
				elaborately designed aspect-aware coarse-tofine generation process. First, we model the
				aspect transitions to capture the overall content
				flow. Then, to generate a sentence, an aspectaware sketch will be predicted using an aspectaware decoder. Finally, another decoder fills in
				the semantic slots by generating corresponding words. Our approach is able to jointly
				utilize aspect semantics, syntactic sketch, and
				context information. Extensive experiments
				results have demonstrated the effectiveness of
				the proposed model.
              </div><br>
              * Corresponding author <br>
	      &dagger; Equal contribution
        </td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Open Source Projects</heading> <br><br>
		(Most of my research work are open-source. Here are some my preferable projects!)
		<ul>
		<li><a href="https://github.com/RUCAIBox/TextBox" target="_black">TextBox</a><br>A unified, comprehensive and efficient framework for reproducing and developing 
			text generation algorithms, covering more than 20 base models and nearly 10 benchmarks.</li>
		</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Professional Services</heading> <br><br>
			<ul>
			<li>Reviewer
				<ul>
					<li>Journal: TALLIP, ACM Computing Survey</li>
					<li>Conference: AAAI 2021-22, IJCAI 2021-22, KDD 2021, EMNLP 2022, COLING 2022</li>
				</ul>
			</li>
			<li>Chair
				<ul>
					<li>CSSNLP 2020 (Co-Chair)</li>
				</ul>
			</li>
			</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Selected Awards and Honors</heading> <br><br>
			<ul>
			<li>National Scholarship for Graduate Student (top 2% students), Ministry of Education of P.R.China, 2021</li>
			<li>SIGIR Student Travel Grant (CIKM 2020)</li>
			<li>National Scholarship for Graduate Student (top 2% students), Ministry of Education of P.R.China, 2019</li>
			<li>China Undergraduate Mathematical Contest in Modeling, Second Prize in Beijing Contest District, 2016</li>	
			</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Education</heading> <br><br>
			<ul>
			<li>Ph.D. student of Artificial Intelligence, Renmin University of China & Universite de Montreal, 2020-present
			</li>
			<li>M.Sc. of Computer Application, Renmin University of China, 2018-2020
			</li>
			<li>B.Sc. of Computer Science, Renmin University of China, 2014-2018
			</li>
			</ul>
	</td></tr>
		
	<tr><td width="100%" valign="middle">
            <heading>Miscellaneous</heading> <br><br>
		<p>
		  <a href="https://jboru.github.io/" style="text-decoration:underline;">Jinhang Jiang</a>&nbsp;&nbsp;&nbsp;
			<a href="https://lancelot39.github.io/" style="text-decoration:underline;">Kun Zhou</a>&nbsp;&nbsp;&nbsp;
			<a href="https://richardhgl.github.io/" style="text-decoration:underline;">Gaole He</a>
		
		</p>
			
	</td></tr><br>
		
	<tr><td width="100%" valign="middle">
	<center>
	<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=571ds4qroi6&amp;s=220&amp;m=7&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
	</center>
	</td></tr>
		
	</tbody></table>
	   
	
	<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-59618557-1', 'auto');
                ga('send', 'pageview');

              </script>

    </td>
    </tr>
  </tbody></table>
  
  <!--footer start-->
 <footer class="footer">
	<p>Copyright 2021. All Rights Reserved by Junyi Li.</p>
 </footer>
     <!--footer end-->
  
</body></html>
